{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Limpieza y vectorización de un libro (PLN)\n",
        "\n",
        "Objetivo: cargar un libro en texto plano (`data/libro.txt`), aplicar **normalización + lematización** (español) y luego **vectorizar** con **BoW** y **TF‑IDF** (con n‑gramas).\n",
        "\n",
        "Si tu entrega requiere *El Principito*, reemplaza el contenido de `data/libro.txt` por el texto del Principito en UTF‑8 (o usa `data/principito.txt` y ajusta `DATA_PATH`).\n",
        "\n",
        "\n"
      ],
      "id": "40396d0d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "\n",
        "ROOT = Path().resolve()\n",
        "DATA_PATH = ROOT / \"data\" / \"libro.txt\"\n",
        "OUTPUT_DIR = ROOT / \"outputs\"\n",
        "SPACY_MODEL = \"es_core_news_sm\"\n",
        "\n",
        "print(\"Proyecto:\", ROOT)\n",
        "print(\"Entrada:\", DATA_PATH)\n",
        "print(\"Salida:\", OUTPUT_DIR)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Cargar texto\n",
        "\n",
        "El archivo debe estar en **UTF‑8**."
      ],
      "id": "883e370b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "text_raw = DATA_PATH.read_text(encoding=\"utf-8\")\n",
        "text_raw = \" \".join(text_raw.split())\n",
        "\n",
        "print(\"Chars:\", len(text_raw))\n",
        "print(text_raw[:400])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Cargar modelo spaCy (español)\n",
        "\n",
        "Si el modelo no está instalado, se descarga automáticamente."
      ],
      "id": "2194715b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    nlp = spacy.load(SPACY_MODEL)\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(SPACY_MODEL)\n",
        "    nlp = spacy.load(SPACY_MODEL)\n",
        "\n",
        "doc = nlp(text_raw)\n",
        "print(\"Tokens spaCy:\", len(doc))\n",
        "print([t.text for t in doc[:20]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Limpieza (normalización + lematización)\n",
        "\n",
        "Reglas:\n",
        "- quitar stopwords\n",
        "- quitar puntuación/espacios/números\n",
        "- quedarse solo con tokens alfabéticos\n",
        "- usar `token.lemma_` en minúsculas"
      ],
      "id": "4309702c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lemmas = []\n",
        "\n",
        "for token in doc:\n",
        "    if token.is_space or token.is_punct or token.like_num:\n",
        "        continue\n",
        "    if token.is_stop:\n",
        "        continue\n",
        "    if not token.is_alpha:\n",
        "        continue\n",
        "\n",
        "    lemma = token.lemma_.lower().strip()\n",
        "    if lemma:\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "print(\"Lemas:\", len(lemmas))\n",
        "print(\"Primeros 30:\", lemmas[:30])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Guardar outputs\n",
        "\n",
        "- `outputs/libro_lemmas.txt`\n",
        "- `outputs/libro_normalizado.txt`\n",
        "- `outputs/top_30_frecuencias.txt`\n",
        "\n",
        "## 5) Vectorización (Checkpoint 2 — Pt. 2)\n",
        "\n",
        "Vectorización por **oraciones lematizadas** usando:\n",
        "- **Bag of Words (BoW)** con n‑gramas\n",
        "- **TF‑IDF** con n‑gramas\n",
        "\n",
        "Y se guardan artefactos:\n",
        "- `outputs/X_bow.npz`, `outputs/X_tfidf.npz`\n",
        "- `outputs/vocab_bow.txt`, `outputs/vocab_tfidf.txt`\n",
        "- `outputs/vectorizacion_meta.json`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "(OUTPUT_DIR / \"libro_lemmas.txt\").write_text(\"\\n\".join(lemmas) + \"\\n\", encoding=\"utf-8\")\n",
        "(OUTPUT_DIR / \"libro_normalizado.txt\").write_text(\" \".join(lemmas) + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "freq = Counter(lemmas)\n",
        "top_30 = freq.most_common(30)\n",
        "(OUTPUT_DIR / \"top_30_frecuencias.txt\").write_text(\n",
        "    \"\\n\".join([f\"{w}\\t{c}\" for w, c in top_30]) + \"\\n\",\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "print(\"Únicos:\", len(freq))\n",
        "print(\"Top 10:\", top_30[:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "import json\n",
        "\n",
        "# Este checkpoint 2 (pt. 2) vectoriza por ORACIONES lematizadas\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "969c7c71"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "corpus_lematizado = []\n",
        "\n",
        "sents = list(doc.sents)\n",
        "if not sents:\n",
        "    # Fallback (raro): si no hay segmentación de oraciones, intentamos con sentencizer\n",
        "    if \"sentencizer\" not in nlp.pipe_names:\n",
        "        nlp.add_pipe(\"sentencizer\")\n",
        "    doc = nlp(text_raw)\n",
        "    sents = list(doc.sents)\n",
        "\n",
        "for oracion in sents:\n",
        "    lemas_oracion = []\n",
        "    for token in oracion:\n",
        "        if token.is_space or token.is_punct or token.like_num:\n",
        "            continue\n",
        "        if token.is_stop:\n",
        "            continue\n",
        "        if not token.is_alpha:\n",
        "            continue\n",
        "        lemma = token.lemma_.lower().strip()\n",
        "        if lemma:\n",
        "            lemas_oracion.append(lemma)\n",
        "\n",
        "    if lemas_oracion:\n",
        "        corpus_lematizado.append(\" \".join(lemas_oracion))\n",
        "\n",
        "print(f\"Total de oraciones procesadas: {len(corpus_lematizado)}\")\n",
        "print(\"Ejemplo:\", corpus_lematizado[0][:160] + (\"...\" if len(corpus_lematizado[0]) > 160 else \"\"))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b6a82f33"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Bag of Words (conteos) con n-gramas\n",
        "bow_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "X_bow = bow_vectorizer.fit_transform(corpus_lematizado)\n",
        "\n",
        "print(\"X_bow:\", X_bow.shape)\n",
        "print(\"Vocab BoW:\", len(bow_vectorizer.get_feature_names_out()))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6478e2ad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TF-IDF (importancia) con n-gramas\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus_lematizado)\n",
        "\n",
        "print(\"X_tfidf:\", X_tfidf.shape)\n",
        "print(\"Vocab TF-IDF:\", len(tfidf_vectorizer.get_feature_names_out()))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f41df109"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (necesario para 3D)\n",
        "\n",
        "\n",
        "def _top_k_by_sum(X, k: int):\n",
        "    scores = np.asarray(X.sum(axis=0)).ravel()\n",
        "    k = min(k, scores.size)\n",
        "    idx = np.argsort(-scores)[:k]\n",
        "    return idx\n",
        "\n",
        "\n",
        "def _top_k_by_mean(X, k: int):\n",
        "    scores = np.asarray(X.mean(axis=0)).ravel()\n",
        "    k = min(k, scores.size)\n",
        "    idx = np.argsort(-scores)[:k]\n",
        "    return idx\n",
        "\n",
        "\n",
        "def _pca_3d_coords(X_words):\n",
        "    pca = PCA(n_components=3, random_state=0)\n",
        "    return pca.fit_transform(X_words)\n",
        "\n",
        "\n",
        "def graficar_palabras_3d(ax, X_oraciones_x_terminos, vocabulario, titulo, color_puntos, idx_terminos):\n",
        "    X_sel = X_oraciones_x_terminos[:, idx_terminos]\n",
        "    vocab_sel = vocabulario[idx_terminos]\n",
        "\n",
        "    # (palabras x oraciones)\n",
        "    X_words = X_sel.T.toarray()\n",
        "    coords = _pca_3d_coords(X_words)\n",
        "\n",
        "    x, y, z = coords[:, 0], coords[:, 1], coords[:, 2]\n",
        "    ax.scatter(x, y, z, c=color_puntos, s=70, edgecolors=\"k\", alpha=0.85, depthshade=True)\n",
        "\n",
        "    for i, palabra in enumerate(vocab_sel):\n",
        "        ax.text(x[i], y[i], z[i] + 0.05, palabra, fontsize=8)\n",
        "\n",
        "    ax.set_title(titulo)\n",
        "    ax.set_xlabel(\"Comp. Principal 1\")\n",
        "    ax.set_ylabel(\"Comp. Principal 2\")\n",
        "    ax.set_zlabel(\"Comp. Principal 3\")\n",
        "\n",
        "\n",
        "top_k = 40\n",
        "\n",
        "fig = plt.figure(figsize=(18, 8))\n",
        "\n",
        "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
        "vocab_bow = bow_vectorizer.get_feature_names_out()\n",
        "idx_bow = _top_k_by_sum(X_bow, top_k)\n",
        "graficar_palabras_3d(ax1, X_bow, vocab_bow, \"Espacio BoW 3D (Top términos)\", \"orange\", idx_bow)\n",
        "\n",
        "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
        "vocab_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "idx_tfidf = _top_k_by_mean(X_tfidf, top_k)\n",
        "graficar_palabras_3d(ax2, X_tfidf, vocab_tfidf, \"Espacio TF-IDF 3D (Top términos)\", \"teal\", idx_tfidf)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fe0924c4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Guardar artefactos de vectorización (sparse matrices + vocabularios)\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "sparse.save_npz(OUTPUT_DIR / \"X_bow.npz\", X_bow)\n",
        "sparse.save_npz(OUTPUT_DIR / \"X_tfidf.npz\", X_tfidf)\n",
        "\n",
        "(OUTPUT_DIR / \"vocab_bow.txt\").write_text(\n",
        "    \"\\n\".join(bow_vectorizer.get_feature_names_out()) + \"\\n\",\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "(OUTPUT_DIR / \"vocab_tfidf.txt\").write_text(\n",
        "    \"\\n\".join(tfidf_vectorizer.get_feature_names_out()) + \"\\n\",\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "meta = {\n",
        "    \"n_oraciones\": len(corpus_lematizado),\n",
        "    \"bow_shape\": list(X_bow.shape),\n",
        "    \"tfidf_shape\": list(X_tfidf.shape),\n",
        "    \"ngram_range\": [1, 2],\n",
        "}\n",
        "(OUTPUT_DIR / \"vectorizacion_meta.json\").write_text(\n",
        "    json.dumps(meta, ensure_ascii=False, indent=2) + \"\\n\",\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "print(\"Guardado en:\", OUTPUT_DIR)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a4b1616a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
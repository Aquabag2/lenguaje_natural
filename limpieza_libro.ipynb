{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Limpieza de un libro en texto plano (PLN)\n",
        "\n",
        "Objetivo: cargar `data/libro.txt` y aplicar **normalización + lematización** (español) usando spaCy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "\n",
        "ROOT = Path().resolve()\n",
        "DATA_PATH = ROOT / \"data\" / \"libro.txt\"\n",
        "OUTPUT_DIR = ROOT / \"outputs\"\n",
        "SPACY_MODEL = \"es_core_news_sm\"\n",
        "\n",
        "print(\"Proyecto:\", ROOT)\n",
        "print(\"Entrada:\", DATA_PATH)\n",
        "print(\"Salida:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Cargar texto\n",
        "\n",
        "El archivo debe estar en **UTF-8**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_raw = DATA_PATH.read_text(encoding=\"utf-8\")\n",
        "text_raw = \" \".join(text_raw.split())\n",
        "\n",
        "print(\"Chars:\", len(text_raw))\n",
        "print(text_raw[:400])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Cargar modelo spaCy (español)\n",
        "\n",
        "Si el modelo no está instalado, se descarga automáticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    nlp = spacy.load(SPACY_MODEL)\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(SPACY_MODEL)\n",
        "    nlp = spacy.load(SPACY_MODEL)\n",
        "\n",
        "doc = nlp(text_raw)\n",
        "print(\"Tokens spaCy:\", len(doc))\n",
        "print([t.text for t in doc[:20]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Limpieza (normalización + lematización)\n",
        "\n",
        "Reglas:\n",
        "- quitar stopwords\n",
        "- quitar puntuación/espacios/números\n",
        "- quedarse solo con tokens alfabéticos\n",
        "- usar `token.lemma_` en minúsculas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lemmas = []\n",
        "\n",
        "for token in doc:\n",
        "    if token.is_space or token.is_punct or token.like_num:\n",
        "        continue\n",
        "    if token.is_stop:\n",
        "        continue\n",
        "    if not token.is_alpha:\n",
        "        continue\n",
        "\n",
        "    lemma = token.lemma_.lower().strip()\n",
        "    if lemma:\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "print(\"Lemas:\", len(lemmas))\n",
        "print(\"Primeros 30:\", lemmas[:30])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Guardar outputs\n",
        "\n",
        "- `outputs/libro_lemmas.txt`\n",
        "- `outputs/libro_normalizado.txt`\n",
        "- `outputs/top_30_frecuencias.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "(OUTPUT_DIR / \"libro_lemmas.txt\").write_text(\"\\n\".join(lemmas) + \"\\n\", encoding=\"utf-8\")\n",
        "(OUTPUT_DIR / \"libro_normalizado.txt\").write_text(\" \".join(lemmas) + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "freq = Counter(lemmas)\n",
        "top_30 = freq.most_common(30)\n",
        "(OUTPUT_DIR / \"top_30_frecuencias.txt\").write_text(\n",
        "    \"\\n\".join([f\"{w}\\t{c}\" for w, c in top_30]) + \"\\n\",\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "print(\"Únicos:\", len(freq))\n",
        "print(\"Top 10:\", top_30[:10])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}


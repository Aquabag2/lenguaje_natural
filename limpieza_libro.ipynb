{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "40396d0d",
      "metadata": {},
      "source": [
        "# Limpieza y vectorización de un libro (PLN)\n",
        "\n",
        "Objetivo: cargar un libro en texto plano (`data/libro.txt`), aplicar **normalización + lematización** (español) y luego **vectorizar** con **BoW** y **TF‑IDF** (con n‑gramas).\n",
        "\n",
        "Si tu entrega requiere *El Principito*, reemplaza el contenido de `data/libro.txt` por el texto del Principito en UTF‑8 (o usa `data/principito.txt` y ajusta `DATA_PATH`).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "\n",
        "ROOT = Path().resolve()\n",
        "DATA_PATH = ROOT / \"data\" / \"libro.txt\"\n",
        "OUTPUT_DIR = ROOT / \"outputs\"\n",
        "SPACY_MODEL = \"es_core_news_sm\"\n",
        "\n",
        "print(\"Proyecto:\", ROOT)\n",
        "print(\"Entrada:\", DATA_PATH)\n",
        "print(\"Salida:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "883e370b",
      "metadata": {},
      "source": [
        "## 1) Cargar texto\n",
        "\n",
        "El archivo debe estar en **UTF‑8**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_raw = DATA_PATH.read_text(encoding=\"utf-8\")\n",
        "text_raw = \" \".join(text_raw.split())\n",
        "\n",
        "print(\"Chars:\", len(text_raw))\n",
        "print(text_raw[:400])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2194715b",
      "metadata": {},
      "source": [
        "## 2) Cargar modelo spaCy (español)\n",
        "\n",
        "Si el modelo no está instalado, se descarga automáticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    nlp = spacy.load(SPACY_MODEL)\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(SPACY_MODEL)\n",
        "    nlp = spacy.load(SPACY_MODEL)\n",
        "\n",
        "doc = nlp(text_raw)\n",
        "print(\"Tokens spaCy:\", len(doc))\n",
        "print([t.text for t in doc[:20]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4309702c",
      "metadata": {},
      "source": [
        "## 3) Limpieza (normalización + lematización)\n",
        "\n",
        "Reglas:\n",
        "- quitar stopwords\n",
        "- quitar puntuación/espacios/números\n",
        "- quedarse solo con tokens alfabéticos\n",
        "- usar `token.lemma_` en minúsculas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lemmas = []\n",
        "\n",
        "for token in doc:\n",
        "    if token.is_space or token.is_punct or token.like_num:\n",
        "        continue\n",
        "    if token.is_stop:\n",
        "        continue\n",
        "    if not token.is_alpha:\n",
        "        continue\n",
        "\n",
        "    lemma = token.lemma_.lower().strip()\n",
        "    if lemma:\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "print(\"Lemas:\", len(lemmas))\n",
        "print(\"Primeros 30:\", lemmas[:30])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3d53bf0",
      "metadata": {},
      "source": [
        "## 4) Guardar outputs\n",
        "\n",
        "- `outputs/libro_lemmas.txt`\n",
        "- `outputs/libro_normalizado.txt`\n",
        "- `outputs/top_30_frecuencias.txt`\n",
        "\n",
        "## 5) Vectorización (Checkpoint 2 — Pt. 2)\n",
        "\n",
        "Vectorización por **oraciones lematizadas** usando:\n",
        "- **Bag of Words (BoW)** con n‑gramas\n",
        "- **TF‑IDF** con n‑gramas\n",
        "\n",
        "Y se guardan artefactos:\n",
        "- `outputs/X_bow.npz`, `outputs/X_tfidf.npz`\n",
        "- `outputs/vocab_bow.txt`, `outputs/vocab_tfidf.txt`\n",
        "- `outputs/vectorizacion_meta.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "(OUTPUT_DIR / \"libro_lemmas.txt\").write_text(\"\\n\".join(lemmas) + \"\\n\", encoding=\"utf-8\")\n",
        "(OUTPUT_DIR / \"libro_normalizado.txt\").write_text(\" \".join(lemmas) + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "freq = Counter(lemmas)\n",
        "top_30 = freq.most_common(30)\n",
        "(OUTPUT_DIR / \"top_30_frecuencias.txt\").write_text(\n",
        "    \"\\n\".join([f\"{w}\\t{c}\" for w, c in top_30]) + \"\\n\",\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "print(\"Únicos:\", len(freq))\n",
        "print(\"Top 10:\", top_30[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "969c7c71",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "import json\n",
        "\n",
        "# Este checkpoint 2 (pt. 2) vectoriza por ORACIONES lematizadas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a82f33",
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_lematizado = []\n",
        "\n",
        "sents = list(doc.sents)\n",
        "if not sents:\n",
        "    # Fallback (raro): si no hay segmentación de oraciones, intentamos con sentencizer\n",
        "    if \"sentencizer\" not in nlp.pipe_names:\n",
        "        nlp.add_pipe(\"sentencizer\")\n",
        "    doc = nlp(text_raw)\n",
        "    sents = list(doc.sents)\n",
        "\n",
        "for oracion in sents:\n",
        "    lemas_oracion = []\n",
        "    for token in oracion:\n",
        "        if token.is_space or token.is_punct or token.like_num:\n",
        "            continue\n",
        "        if token.is_stop:\n",
        "            continue\n",
        "        if not token.is_alpha:\n",
        "            continue\n",
        "        lemma = token.lemma_.lower().strip()\n",
        "        if lemma:\n",
        "            lemas_oracion.append(lemma)\n",
        "\n",
        "    if lemas_oracion:\n",
        "        corpus_lematizado.append(\" \".join(lemas_oracion))\n",
        "\n",
        "print(f\"Total de oraciones procesadas: {len(corpus_lematizado)}\")\n",
        "print(\"Ejemplo:\", corpus_lematizado[0][:160] + (\"...\" if len(corpus_lematizado[0]) > 160 else \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6478e2ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bag of Words (conteos) con n-gramas\n",
        "bow_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "X_bow = bow_vectorizer.fit_transform(corpus_lematizado)\n",
        "\n",
        "print(\"X_bow:\", X_bow.shape)\n",
        "print(\"Vocab BoW:\", len(bow_vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f41df109",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF (importancia) con n-gramas\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus_lematizado)\n",
        "\n",
        "print(\"X_tfidf:\", X_tfidf.shape)\n",
        "print(\"Vocab TF-IDF:\", len(tfidf_vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0924c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (necesario para 3D)\n",
        "\n",
        "\n",
        "def _top_k_by_sum(X, k: int):\n",
        "    scores = np.asarray(X.sum(axis=0)).ravel()\n",
        "    k = min(k, scores.size)\n",
        "    idx = np.argsort(-scores)[:k]\n",
        "    return idx\n",
        "\n",
        "\n",
        "def _top_k_by_mean(X, k: int):\n",
        "    scores = np.asarray(X.mean(axis=0)).ravel()\n",
        "    k = min(k, scores.size)\n",
        "    idx = np.argsort(-scores)[:k]\n",
        "    return idx\n",
        "\n",
        "\n",
        "def _pca_3d_coords(X_words):\n",
        "    pca = PCA(n_components=3, random_state=0)\n",
        "    return pca.fit_transform(X_words)\n",
        "\n",
        "\n",
        "def graficar_palabras_3d(ax, X_oraciones_x_terminos, vocabulario, titulo, color_puntos, idx_terminos):\n",
        "    X_sel = X_oraciones_x_terminos[:, idx_terminos]\n",
        "    vocab_sel = vocabulario[idx_terminos]\n",
        "\n",
        "    # (palabras x oraciones)\n",
        "    X_words = X_sel.T.toarray()\n",
        "    coords = _pca_3d_coords(X_words)\n",
        "\n",
        "    x, y, z = coords[:, 0], coords[:, 1], coords[:, 2]\n",
        "    ax.scatter(x, y, z, c=color_puntos, s=70, edgecolors=\"k\", alpha=0.85, depthshade=True)\n",
        "\n",
        "    for i, palabra in enumerate(vocab_sel):\n",
        "        ax.text(x[i], y[i], z[i] + 0.05, palabra, fontsize=8)\n",
        "\n",
        "    ax.set_title(titulo)\n",
        "    ax.set_xlabel(\"Comp. Principal 1\")\n",
        "    ax.set_ylabel(\"Comp. Principal 2\")\n",
        "    ax.set_zlabel(\"Comp. Principal 3\")\n",
        "\n",
        "\n",
        "top_k = 40\n",
        "\n",
        "fig = plt.figure(figsize=(18, 8))\n",
        "\n",
        "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
        "vocab_bow = bow_vectorizer.get_feature_names_out()\n",
        "idx_bow = _top_k_by_sum(X_bow, top_k)\n",
        "graficar_palabras_3d(ax1, X_bow, vocab_bow, \"Espacio BoW 3D (Top términos)\", \"orange\", idx_bow)\n",
        "\n",
        "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
        "vocab_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "idx_tfidf = _top_k_by_mean(X_tfidf, top_k)\n",
        "graficar_palabras_3d(ax2, X_tfidf, vocab_tfidf, \"Espacio TF-IDF 3D (Top términos)\", \"teal\", idx_tfidf)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4b1616a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar artefactos de vectorización (sparse matrices + vocabularios)\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "sparse.save_npz(OUTPUT_DIR / \"X_bow.npz\", X_bow)\n",
        "sparse.save_npz(OUTPUT_DIR / \"X_tfidf.npz\", X_tfidf)\n",
        "\n",
        "(OUTPUT_DIR / \"vocab_bow.txt\").write_text(\n",
        "    \"\\n\".join(bow_vectorizer.get_feature_names_out()) + \"\\n\",\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "(OUTPUT_DIR / \"vocab_tfidf.txt\").write_text(\n",
        "    \"\\n\".join(tfidf_vectorizer.get_feature_names_out()) + \"\\n\",\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "meta = {\n",
        "    \"n_oraciones\": len(corpus_lematizado),\n",
        "    \"bow_shape\": list(X_bow.shape),\n",
        "    \"tfidf_shape\": list(X_tfidf.shape),\n",
        "    \"ngram_range\": [1, 2],\n",
        "}\n",
        "(OUTPUT_DIR / \"vectorizacion_meta.json\").write_text(\n",
        "    json.dumps(meta, ensure_ascii=False, indent=2) + \"\\n\",\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "print(\"Guardado en:\", OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "048bb09f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Semántica distribucional (Word2Vec)\n",
        "# Entrenamos embeddings (semántica distribucional) sobre oraciones lematizadas.\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "FIGURES_DIR = ROOT / \"figures\"\n",
        "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "sentences_tokens = [s.split() for s in corpus_lematizado]\n",
        "\n",
        "w2v = Word2Vec(\n",
        "    sentences=sentences_tokens,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=2,\n",
        "    sg=1,  # skip-gram\n",
        "    workers=4,\n",
        "    seed=42,\n",
        "    epochs=50,\n",
        ")\n",
        "\n",
        "print(\"Vocab Word2Vec:\", len(w2v.wv.index_to_key))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b13e2c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selección de palabras a graficar (top por frecuencia, presentes en el vocab)\n",
        "from collections import Counter\n",
        "\n",
        "freq_w = Counter(w for sent in sentences_tokens for w in sent)\n",
        "\n",
        "words = [w for w, _ in freq_w.most_common(60) if w in w2v.wv][:60]\n",
        "vectors = np.vstack([w2v.wv[w] for w in words])\n",
        "\n",
        "print(\"Palabras graficadas:\", len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f10ddc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (A) PCA 2D del espacio Word2Vec\n",
        "pca2 = PCA(n_components=2, random_state=42)\n",
        "coords_pca = pca2.fit_transform(vectors)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.scatter(coords_pca[:, 0], coords_pca[:, 1], s=35, alpha=0.85, edgecolors=\"k\", linewidths=0.3)\n",
        "for i, w in enumerate(words):\n",
        "    plt.text(coords_pca[i, 0], coords_pca[i, 1], w, fontsize=9)\n",
        "plt.title(\"Word2Vec (Skip-gram) — PCA 2D\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / \"word2vec_pca_2d.png\", dpi=200)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a1a1e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (B) t-SNE 2D del espacio Word2Vec\n",
        "perplexity = min(30, max(5, (len(words) - 1) // 3))\n",
        "tsne2 = TSNE(\n",
        "    n_components=2,\n",
        "    random_state=42,\n",
        "    init=\"pca\",\n",
        "    learning_rate=\"auto\",\n",
        "    perplexity=perplexity,\n",
        ")\n",
        "coords_tsne = tsne2.fit_transform(vectors)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.scatter(coords_tsne[:, 0], coords_tsne[:, 1], s=35, alpha=0.85, edgecolors=\"k\", linewidths=0.3)\n",
        "for i, w in enumerate(words):\n",
        "    plt.text(coords_tsne[i, 0], coords_tsne[i, 1], w, fontsize=9)\n",
        "plt.title(f\"Word2Vec (Skip-gram) — t-SNE 2D (perplexity={perplexity})\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / \"word2vec_tsne_2d.png\", dpi=200)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3f832a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar TXT del texto procesado (versionable)\n",
        "PROCESSED_TXT = ROOT / \"data\" / \"texto_procesado.txt\"\n",
        "PROCESSED_TXT.write_text(\"\\n\".join(corpus_lematizado) + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "print(\"TXT procesado:\", PROCESSED_TXT)\n",
        "print(\"Imágenes en:\", FIGURES_DIR)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

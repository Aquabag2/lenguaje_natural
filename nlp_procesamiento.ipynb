{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Haciendo leer un libro a tu computadora (PLN) — Parte 1\n",
        "\n",
        "Este notebook carga un libro en `.txt` y aplica limpieza:\n",
        "\n",
        "- Normalización básica (espacios, minúsculas)\n",
        "- Filtrado (stopwords, puntuación, números)\n",
        "- **Lematización** en español con spaCy\n",
        "\n",
        "Entrada: `data/libro.txt`\n",
        "Salidas: carpeta `outputs/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "\n",
        "ROOT = Path().resolve()\n",
        "DATA_PATH = ROOT / \"data\" / \"libro.txt\"\n",
        "OUTPUT_DIR = ROOT / \"outputs\"\n",
        "SPACY_MODEL = \"es_core_news_sm\"\n",
        "\n",
        "print(\"Proyecto:\", ROOT)\n",
        "print(\"Entrada:\", DATA_PATH)\n",
        "print(\"Salida:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Cargar texto\n",
        "\n",
        "El archivo debe estar en **UTF-8**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_raw = DATA_PATH.read_text(encoding=\"utf-8\")\n",
        "text_raw = \" \".join(text_raw.split())\n",
        "\n",
        "print(\"Chars:\", len(text_raw))\n",
        "print(text_raw[:400])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Cargar spaCy en español\n",
        "\n",
        "Si el modelo no está instalado, se descarga automáticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    nlp = spacy.load(SPACY_MODEL)\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(SPACY_MODEL)\n",
        "    nlp = spacy.load(SPACY_MODEL)\n",
        "\n",
        "doc = nlp(text_raw)\n",
        "print(\"Tokens spaCy:\", len(doc))\n",
        "print([t.text for t in doc[:20]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Limpieza: filtrado + lematización\n",
        "\n",
        "Reglas aplicadas:\n",
        "- quitar stopwords\n",
        "- quitar puntuación, espacios y números\n",
        "- quedarse solo con tokens alfabéticos\n",
        "- usar lema en minúsculas (`token.lemma_`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lemmas = []\n",
        "\n",
        "for token in doc:\n",
        "    if token.is_space or token.is_punct or token.like_num:\n",
        "        continue\n",
        "    if token.is_stop:\n",
        "        continue\n",
        "    if not token.is_alpha:\n",
        "        continue\n",
        "\n",
        "    lemma = token.lemma_.lower().strip()\n",
        "    if lemma:\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "print(\"Lemas:\", len(lemmas))\n",
        "print(\"Primeros 30:\", lemmas[:30])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Guardar resultados\n",
        "\n",
        "- `outputs/libro_lemmas.txt`: 1 lema por línea\n",
        "- `outputs/libro_normalizado.txt`: texto normalizado (lemmas concatenados)\n",
        "- `outputs/top_30_frecuencias.txt`: conteos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "(OUTPUT_DIR / \"libro_lemmas.txt\").write_text(\"\\n\".join(lemmas) + \"\\n\", encoding=\"utf-8\")\n",
        "(OUTPUT_DIR / \"libro_normalizado.txt\").write_text(\" \".join(lemmas) + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "freq = Counter(lemmas)\n",
        "top_30 = freq.most_common(30)\n",
        "(OUTPUT_DIR / \"top_30_frecuencias.txt\").write_text(\n",
        "    \"\\n\".join([f\"{w}\\t{c}\" for w, c in top_30]) + \"\\n\",\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "\n",
        "print(\"Únicos:\", len(freq))\n",
        "print(\"Top 10:\", top_30[:10])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cargar modelo ahora en español \n",
        "try:\n",
        "    nlp = spacy.load(\"es_core_news_sm\")\n",
        "except OSError:\n",
        "    print(\"Descargando modelo...\")\n",
        "    from spacy.cli import download\n",
        "    download(\"es_core_news_sm\")\n",
        "    nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# TEXTO DE ENTRADA \n",
        "with open(\"cap1_principito.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texto_principito = f.read()\n",
        "\n",
        "# Ahora 'texto_principito' contiene todo el texto del archivo\n",
        "print(f\"Texto cargado con éxito. Longitud: {len(texto_principito)} caracteres.\")\n",
        "\n",
        "# 1. TOKENIZACIÓN\n",
        "# SpaCy procesa el texto y crea el objeto 'doc' lleno de metadatos\n",
        "doc = nlp(texto_principito)\n",
        "\n",
        "# Mostrar los primeros 15 tokens para entender cómo \"ve\" la máquina el texto\n",
        "print(f\"--- 1. Tokenización (Total tokens: {len(doc)}) ---\")\n",
        "print([token.text for token in doc][:20]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. FILTRADO DE STOP WORDS\n",
        "# Separamos lo que aporta valor semántico del \"pegamento\" gramatical\n",
        "\n",
        "tokens_relevantes = []\n",
        "tokens_ruido = []\n",
        "\n",
        "for token in doc:\n",
        "    # Filtramos si es stopword o si es puntuación\n",
        "    if not token.is_stop and not token.is_punct and token.text.strip():\n",
        "        tokens_relevantes.append(token.text)\n",
        "    elif token.is_stop:\n",
        "        tokens_ruido.append(token.text)\n",
        "\n",
        "print(f\"\\n--- 2. Filtrado de Stop Words ---\")\n",
        "print(f\"Palabras eliminadas (Ruido): {tokens_ruido[:10]}...\")\n",
        "print(f\"Palabras conservadas (Contenido): {tokens_relevantes[:10]}...\")\n",
        "print(f\"Reducción de tamaño: de {len(doc)} a {len(tokens_relevantes)} tokens.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. LEMATIZACIÓN Y NORMALIZACIÓN FINAL\n",
        "# Reducimos las palabras a su raíz (Lema) y estandarizamos a minúsculas\n",
        "# Objetivo: Que \"hablo\", \"hablaré\" y \"habla\" cuenten como el mismo concepto: \"hablar\"\n",
        "\n",
        "tokens_normalizados = []\n",
        "cambios_interesantes = []\n",
        "\n",
        "for token in doc:\n",
        "    # Aplicamos los mismos filtros de calidad que en el paso 2\n",
        "    if not token.is_stop and not token.is_punct and token.text.strip():\n",
        "        \n",
        "        # AQUÍ OCURRE LA MAGIA: \n",
        "        # 1. Extraemos el lema (token.lemma_)\n",
        "        # 2. Convertimos a minúsculas (.lower())\n",
        "        lema = token.lemma_.lower()\n",
        "        tokens_normalizados.append(lema)\n",
        "        \n",
        "        # Para fines educativos: Guardamos casos donde la palabra cambió drásticamente\n",
        "        # Ej: \"fui\" -> \"ir\"\n",
        "        if token.text.lower() != lema:\n",
        "            cambios_interesantes.append(f\"{token.text} ➡ {lema}\")\n",
        "\n",
        "print(f\"\\n--- 3. Lematización y Normalización ---\")\n",
        "print(f\"Total de tokens procesados: {len(tokens_normalizados)}\")\n",
        "print(f\"Ejemplos de transformaciones (Palabra original ➡ Lema):\")\n",
        "# Mostramos solo los primeros 5 cambios para no saturar la pantalla\n",
        "print(cambios_interesantes[:10]) \n",
        "\n",
        "print(f\"\\nResultado final (Primeros 10 tokens):\")\n",
        "print(tokens_normalizados[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Configuración del Stemmer (NLTK)\n",
        "stemmer = SnowballStemmer(\"spanish\")\n",
        "\n",
        "# Comparativa lado a lado\n",
        "data_comparativa = []\n",
        "\n",
        "for token in doc:\n",
        "    # Solo analizamos palabras, ignoramos puntuación y espacios para claridad\n",
        "    if not token.is_punct and not token.is_space:\n",
        "        \n",
        "        # A. STEMMING (Corte de sufijos)\n",
        "        raiz_stem = stemmer.stem(token.text)\n",
        "        \n",
        "        # B. LEMATIZACIÓN (Análisis morfológico de SpaCy)\n",
        "        # Nota: SpaCy usa el contexto para saber si 'fui' es 'ir' o 'ser'\n",
        "        lema = token.lemma_\n",
        "        \n",
        "        data_comparativa.append({\n",
        "            \"Original\": token.text,\n",
        "            \"Stemming (Corte)\": raiz_stem,\n",
        "            \"Lematización (Diccionario)\": lema,\n",
        "            \"¿Coinciden?\": raiz_stem == lema\n",
        "        })\n",
        "\n",
        "# Crear DataFrame para visualizar\n",
        "df = pd.DataFrame(data_comparativa)\n",
        "\n",
        "print(f\"\\n--- 3. Stemming vs Lematización ---\")\n",
        "# Mostramos palabras interesantes donde se vea la diferencia\n",
        "# Buscamos verbos conjugados o plurales\n",
        "palabras_interesantes = [\"hombres\", \"olvidado\", \"eres\", \"domesticado\", \"invisible\", \"ojos\"]\n",
        "filtro = df[df[\"Original\"].isin(palabras_interesantes)]\n",
        "\n",
        "print(filtro.to_string(index=False))\n",
        "\n",
        "print(\"\\n--- Visualización completa de los primeros 10 tokens ---\")\n",
        "print(df.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a907035",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5422b019",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
